esting# Large Dataset Generator for Chinook Database

This Python script generates realistic large-scale data for the Chinook database with support for direct database insertion and database size inflation.

## Features

- **Quick Mode**: Run with `--quick` flag to use sensible defaults and skip all prompts
- **Direct Database Insertion**: Insert data directly to SQL Server (faster than executing large SQL files)
- **Database Size Inflation**: Optional SystemLog table with configurable padding (~50KB per row) for subsetting demonstrations
- **Realistic Data**: Uses real names from diverse cultures (200+ first names, 200+ last names)
  - Middle initials (40% of customers)
  - Suffixes like Jr., Sr., III (5% of customers)
- **Geographic Accuracy**: Accurate city/country/state combinations with proper postal codes
- **Real Artists**: 80 chart-topping artists with 160 albums and 439 tracks (clean content only)
- **Date Range**: Invoices from January 1, 2022 to February 12, 2026 (4+ years of data)
- **Realistic Invoice Addresses**: 90% of invoices use customer's actual billing address
- **Multi-Database Support**: Generates platform-specific SQL for:
  - SQL Server (MSSQL) - with direct insertion support
  - Oracle
  - PostgreSQL
  - MySQL
- **Real-Time Progress Tracking**: Shows progress with timestamps during insertion
- **Optimized for Large Datasets**: Features batching (1000 rows per INSERT), transactions, and efficient memory usage
- **Scalable**: Successfully tested with 10,000+ customers and 100,000+ invoices

## Quick Start

### Fastest Way (Recommended for Testing)

Run with all defaults - generates and inserts data immediately:

```bash
python Chinook_GenerateData.py --quick
```

**Defaults:**
- Database: SQL Server (localhost/Chinook_FullRestore)
- Authentication: Windows Authentication
- Customers: 941 (total: 1,000)
- Invoices: 3,588 (total: 4,000)
- SystemLog: 10,000 rows (~500MB for subsetting demos)
- Mode: Direct database insertion

### Quick Mode with Custom Values

```bash
# Custom SystemLog rows
python Chinook_GenerateData.py --quick --systemlog 20000

# Large dataset with 1GB SystemLog
python Chinook_GenerateData.py --quick --customers 10000 --invoices 100000 --systemlog 20000

# Skip SystemLog entirely
python Chinook_GenerateData.py --quick --systemlog 0

# Custom everything
python Chinook_GenerateData.py --quick --customers 5000 --invoices 50000 --systemlog 5000
```

**SystemLog Sizing Guide:**
- ~13,000 rows ≈ 100MB
- ~65,000 rows ≈ 500MB (default for quick mode)
- ~130,000 rows ≈ 1GB

## Usage

### Interactive Mode

Run the script without arguments for full control over all options:

```bash
python Chinook_GenerateData.py
```

You'll be prompted to select:
1. **Target database** - SQL Server, Oracle, PostgreSQL, MySQL, or All
2. **Insertion mode** (SQL Server only):
   - Generate SQL file only
   - Direct database insert (faster, with real-time progress)
3. **Connection details** (for direct insert):
   - Server name (default: localhost)
   - Database name (default: Chinook_FullRestore)
   - Authentication: Windows Auth or SQL Server Auth
4. **Number of customers** - Default: 941 (total: 1,000)
5. **Number of invoices** - Default: 3,588 (total: 4,000)
6. **SystemLog rows** - Default: 0 (enter number for database size inflation)

### Command Line Mode (File Generation Only)

```bash
# Generate for all databases with default counts
python Chinook_GenerateData.py all

# Generate for specific database
python Chinook_GenerateData.py mssql
python Chinook_GenerateData.py oracle
python Chinook_GenerateData.py postgresql
python Chinook_GenerateData.py mysql

# Specify custom counts: database customers invoices
python Chinook_GenerateData.py all 500 2000
python Chinook_GenerateData.py mssql 10000 100000
```

## SystemLog Table for Database Size Inflation

The SystemLog feature allows you to artificially inflate your database size for subsetting demonstrations. Each log entry:

- References a valid InvoiceId (maintains referential integrity)
- Contains realistic log messages about invoice processing
- Has ~50KB of padding data generated by SQL Server (using REPLICATE function)
- Inserts quickly despite large size (SQL generates padding during insert)

**Use Cases:**
- Demonstrate database subsetting tools (exclude large "junk" tables)
- Test backup/restore performance with larger databases
- Show storage optimization benefits

**Requirements:**
- SystemLog table must exist in database (uses FK to Invoice table)
- Only inserts if table exists (gracefully skips if missing)

**Example:**
```bash
# Generate 500MB of SystemLog data (10,000 rows)
python Chinook_GenerateData.py --quick --systemlog 10000

# Generate 1GB of SystemLog data (20,000 rows)
python Chinook_GenerateData.py --quick --systemlog 20000
```

## Performance Optimizations

The script includes several optimizations for handling large datasets:

1. **Direct Database Insertion**: Uses `sqlcmd` utility for SQL Server (handles GO statements, real-time progress)
2. **Batched Inserts**: All databases use batched INSERT statements (1000 rows per batch for MSSQL/PostgreSQL/MySQL, 500 for Oracle)
3. **Transaction Boundaries**: All INSERTs wrapped in single atomic transaction
4. **Explicit ID Management**: Uses `IDENTITY_INSERT` (MSSQL) and explicit IDs for all databases to ensure proper foreign key relationships
5. **Memory Efficient**: Processes data incrementally, streaming writes to avoid memory issues
6. **Progress Tracking**: Real-time progress messages with timestamps every 10 batches
7. **SQL-Generated Padding**: SystemLog padding generated by database (not Python) for minimal memory footprint

## Output Files

The script creates database-specific SQL files in their respective directories:

```
Database/
├── MSSQL/
│   └── large_dataset_inserts_mssql.sql
├── Oracle/
│   └── large_dataset_inserts_oracle.sql
├── PostgreSQL/
│   └── large_dataset_inserts_postgresql.sql
└── MySQL/
    └── large_dataset_inserts_mysql.sql
```

**Note**: In direct insertion mode (SQL Server only), the SQL file is generated first, then executed automatically via `sqlcmd`. The file remains for reference or manual re-execution.

Each file contains INSERT statements compatible with that platform's syntax:
- **SQL Server**: `[dbo].[Table]`, `N'string'` literals, `IDENTITY_INSERT` management, optional SystemLog with SQL-generated padding
- **Oracle**: `INSERT ALL...SELECT FROM dual`, explicit IDs
- **PostgreSQL**: lowercase `table_name`, `TIMESTAMP` format
- **MySQL**: backtick identifiers `` `Table` ``

## Generated Data

### Customers (Default: 941 new, 1,000 total)
- 59 original customers in base database
- Diverse names from American, European, Latin American, Indian, Asian, Middle Eastern, and Slavic cultures
- **Name variety**: 40% have middle initials, 5% have suffixes (Jr., Sr., III, II, IV)
- Geographic accuracy: Real cities matched with appropriate countries and states
- Realistic postal codes, phone numbers with country-appropriate prefixes
- Email addresses with country-specific domains (.com, .co.uk, .de, .in, etc.)

### Invoices (Default: 3,588 new, 4,000 total)
- 412 original invoices in base database
- Date range: January 1, 2022 - February 12, 2026 (4+ years of historical data)
- Each invoice has 1-10 invoice lines (tracks purchased)
- **Realistic addresses**: 90% match customer's billing address, 10% use alternate addresses (gifts, work)
- Random amounts calculated from actual track prices and quantities

### Invoice Lines
- Links invoices to tracks (realistic purchases)
- 1-10 tracks per invoice
- Quantities: 1-3 per track
- Uses actual track prices from Track table

### Artists & Music (80 artists, 160 albums, 439 tracks)
- Real chart-topping artists (Taylor Swift, Ed Sheeran, Coldplay, etc.)
- Two albums per artist with realistic track counts
- Clean content only (no explicit lyrics)
- Covers Pop, Rock, Hip-Hop, Country, and EDM genres

### SystemLog (Optional, for database size inflation)
- Realistic invoice processing log messages
- Each row references a valid InvoiceId (FK constraint maintained)
- ~7.8KB padding per row (generated by SQL Server during insertion)
- Timestamps match invoice date range (2022-2026)
- **Sizing**: ~13,000 rows ≈ 100MB | ~65,000 rows ≈ 500MB | ~130,000 rows ≈ 1GB

## Requirements

- Python 3.7 or higher
- No external Python dependencies (uses only standard library)
- **For direct SQL Server insertion**:
  - `sqlcmd` utility (included with SQL Server, SQL Server Management Studio, or SQL Server Command Line Utilities)
  - Windows Authentication or SQL Server Authentication credentials
  - Network access to target SQL Server instance

## Example Output (Quick Mode)

```
================================================================================
Chinook Database - Large Scale Data Generator
================================================================================

QUICK MODE - Using defaults with command-line options

Database: SQL Server (localhost/Chinook_FullRestore)
Mode: Direct insertion (Windows Authentication)
Customers: 941
Invoices: 3,588
SystemLog: 10,000 rows (~500MB)

Testing SQL Server connection...
✓ Connection successful

Generating 1,000 customers with diverse, realistic data...

✓ Generated 941 new customers
  Total customers: 1,000 (59 original + 941 new)

Generating 4,000 invoices for 2022-2026 (Jan 1, 2022 - Feb 12, 2026)...

✓ Generated 3,588 new invoices
✓ Generated 35,432 new invoice lines
  Total invoices: 4,000 (412 original + 3,588 new)

Generating 200 real artists from charts with clean content...

✓ Generated 80 artists
✓ Generated 160 albums
✓ Generated 439 tracks

Generating 10,000 SystemLog entries (~78MB after SQL padding)...

✓ Generated 10,000 log entries (SQL Server will add ~7.8KB padding per row)

Generating SQL file: MSSQL/large_dataset_inserts_mssql.sql...
  ✓ SQL file generated

Executing SQL file using sqlcmd...

[2026-02-12 14:30:15] Starting data insertion
[2026-02-12 14:30:16] Inserting artists...
[2026-02-12 14:30:17] Inserting albums...
[2026-02-12 14:30:18] Inserting tracks...
[2026-02-12 14:30:19] Inserting customers... batch 1 of 1
[2026-02-12 14:30:23] Inserting invoices... batch 1 of 4
[2026-02-12 14:30:28] Inserting invoice lines... batch 1 of 36
[2026-02-12 14:30:45] Inserting system log entries... batch 1 of 10
[2026-02-12 14:31:02] Committing transaction...
[2026-02-12 14:31:05] Data insertion completed successfully!

Execution completed in 50.2 seconds (0.8 minutes)

✓ Data inserted successfully to localhost/Chinook_FullRestore
```

## Example Output (Interactive Mode)

## Example Output (Interactive Mode)

```
================================================================================
Chinook Database - Large Scale Data Generator
================================================================================

Select target database(s):
  1. SQL Server
  2. Oracle
  3. PostgreSQL
  4. MySQL
  5. All databases

Enter choice (1-5): 5

How many new customers to generate?
  Current: 59 in base database
  Recommended: 941 (for total of 1,000)
Enter number of new customers (default 941): 

How many new invoices to generate?
  Current: 412 in base database
  Recommended: 3,588 (for total of 4,000)
  Date range: Jan 1, 2022 - Feb 12, 2026
Enter number of new invoices (default 3588): 

Generate SystemLog entries for database size inflation?
  SQL Server generates ~7.8KB padding per row during insertion (fast & memory-efficient)
  ~13,000 rows ≈ 100MB | ~65,000 rows ≈ 500MB | ~130,000 rows ≈ 1GB
How many SystemLog rows to generate? (0 to skip, default: 0): 10000

Generating 1,000 customers with diverse, realistic data...

✓ Generated 941 new customers
  Total customers: 1,000 (59 original + 941 new)

Generating 4,000 invoices for 2022-2026 (Jan 1, 2022 - Feb 12, 2026)...

✓ Generated 3,588 new invoices
✓ Generated 35,432 new invoice lines
  Total invoices: 4,000 (412 original + 3,588 new)

Generating 200 real artists from charts with clean content...

✓ Generated 80 artists
✓ Generated 160 albums
✓ Generated 439 tracks

Generating 10,000 SystemLog entries (~78MB after SQL padding)...

✓ Generated 10,000 log entries (SQL Server will add ~7.8KB padding per row)

Creating MSSQL format...
  ✓ MSSQL/large_dataset_inserts_mssql.sql
Creating ORACLE format...
  ✓ Oracle/large_dataset_inserts_oracle.sql
Creating POSTGRESQL format...
  ✓ PostgreSQL/large_dataset_inserts_postgresql.sql
Creating MYSQL format...
  ✓ MySQL/large_dataset_inserts_mysql.sql

================================================================================
Summary:
  - 80 real artists from Billboard/mainstream charts
  - 160 real albums with clean content
  - 439 real tracks (no explicit content)
  - 941 realistic customers from diverse cultures (40% with middle initials, 5% with suffixes)
  - Accurate city/country/state combinations
  - 3,588 invoices (Jan 1, 2022 - Feb 12, 2026)
  - 35,432 invoice lines (1-10 tracks per invoice)
  - 10,000 SystemLog entries (~500MB for subsetting demos)
  - Realistic invoice amounts calculated from track prices
  - 90% of invoices use customer's actual billing address
  - Generated for: MSSQL, ORACLE, POSTGRESQL, MYSQL
================================================================================
```

## Use Cases

1. **Quick Testing**: Use `--quick` mode for instant data generation with sensible defaults
2. **Performance Testing**: Test application performance with thousands of records
3. **Database Subsetting**: Use SystemLog to demonstrate excluding large "junk" tables
4. **Demo Scenarios**: Realistic data for demonstrations and training
5. **Migration Testing**: Test database migrations with substantial datasets
6. **Reporting**: Test report generation with meaningful data volumes (4+ years of invoices)
7. **Multi-Database Compatibility**: Verify application works across all supported databases
8. **Direct Insertion**: Skip manual SQL file execution with direct database insertion

## Tips

- **Quick Mode**: Best for development and testing - uses sensible defaults
- **Direct Insertion**: Faster than executing large SQL files manually in SSMS
- **SystemLog Sizing**: Start with 10,000 rows (~500MB) for subsetting demonstrations
- **Large Datasets**: For 100K+ invoices, use direct insertion mode for real-time progress
- **Progress Tracking**: Watch for timestamped messages every 10 batches during insertion
- **Memory**: SystemLog padding is generated by SQL Server (not Python), so no memory issues
